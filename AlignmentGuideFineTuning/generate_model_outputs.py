import os
import json
import sys
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from tqdm import tqdm
sys.stdout.reconfigure(line_buffering=True)  # âœ… æ’åœ¨è¿™é‡Œ
# ====== é…ç½® ======
base_path = "/root/AAAI2026/AlignmentGuideFineTuning"
data_path = os.path.join(base_path, "finetune_data_poi.jsonl")
output_dir = base_path
max_length = 512

# ====== æ¨¡å‹æ˜ å°„è¡¨ ======
model_pairs = {
    "gemma": ("gemma-7b", "finetuned_gemma_7b"),
    "mistral": ("Mistral-7B-Instruct-v0.3", "finetuned_mistral_7b_instruct"),
    "internlm2": ("InternLM2-7B-Chat", "finetuned_internlm2_7b"),
    "qwen2.5": ("Qwen2.5-7B", "finetuned_qwen2.5_7b"),
    "deepseek": ("deepseek-llm-7b-chat", "finetuned_deepseek_7b_chat"),
}

# ====== åŠ è½½æ•°æ® ======
with open(data_path, "r") as f:
    prompts = [json.loads(line.strip()) for line in f]

# ====== æ¨ç†å‡½æ•° ======
def generate_response(model, tokenizer, prompt_text):
    inputs = tokenizer(prompt_text, return_tensors="pt").to("cuda")
    outputs = model.generate(
        **inputs,
        max_new_tokens=max_length,
        do_sample=False,
        temperature=0.7,
        top_p=0.95,
        pad_token_id=tokenizer.eos_token_id
    )
    return tokenizer.decode(outputs[0], skip_special_tokens=True).strip()

# ====== æŒ‰é˜¶æ®µç”Ÿæˆï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰ ======
for stage in ["tuned", "orig"]:  # orig = åŸå§‹æ¨¡å‹ï¼›tuned = å¾®è°ƒæ¨¡å‹
    for model_name, (orig_path, tuned_path) in model_pairs.items():
        print(f"\nğŸ” Stage: {stage.upper()}, Model: {model_name}")

        model_path = os.path.join(base_path, orig_path if stage == "orig" else tuned_path)
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True,
            torch_dtype=torch.float16
        ).to("cuda")
        model.eval()

        for i, item in tqdm(enumerate(prompts, 1), total=len(prompts)):
            idx = f"{i:04d}"  # ç”¨å››ä½æ•°ç¼–å·ï¼Œé¿å… 1, 10, 100 æ··ä¹±
            intent = item["intent"].strip()
            prompt_text = item["prompt"].strip()
            input_text = f"{intent}\n{prompt_text}"

            output_name = f"Answer{idx}_POI_{'finetuned_' if stage == 'tuned' else ''}{model_name}.md"
            output_path = os.path.join(output_dir, output_name)

            # â›” å·²å­˜åœ¨åˆ™è·³è¿‡
            if os.path.exists(output_path):
                continue

            output = generate_response(model, tokenizer, input_text)

            with open(output_path, "w") as f:
                f.write(f"### Intent:\n{intent}\n\n### Prompt:\n{prompt_text}\n\n### Generated Code:\n{output}\n")

         # æ¸…ç†æ˜¾å­˜
        del model
        torch.cuda.empty_cache()

# ====== ç§»åŠ¨æ‰€æœ‰ç”Ÿæˆçš„ .md æ–‡ä»¶åˆ° modeloutputs æ–‡ä»¶å¤¹ ======
import shutil

model_output_dir = os.path.join(base_path, "modeloutputs")
os.makedirs(model_output_dir, exist_ok=True)

for filename in os.listdir(base_path):
    if filename.startswith("Answer") and filename.endswith(".md"):
        src_path = os.path.join(base_path, filename)
        dst_path = os.path.join(model_output_dir, filename)
        shutil.move(src_path, dst_path)
        print(f"Moved: {filename}")

print("\nâœ… All .md files generated and moved to modeloutputs/")