import json
import torch
import csv
import os
from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling
from datasets import Dataset
from torch.utils.data import DataLoader
from peft import LoraConfig, get_peft_model
from accelerate import Accelerator
from torch.optim import AdamW

# ========== 1. åˆå§‹åŒ– Accelerate ==========
accelerator = Accelerator(
    mixed_precision="bf16",  # A10 ä½¿ç”¨ bf16 æ€§èƒ½æ›´ä¼˜
    gradient_accumulation_steps=16
)

# ========== 2. åŠ è½½æœ¬åœ°æ¨¡å‹ä¸åˆ†è¯å™¨ ==========
model_path = "/root/AAAI2026/AlignmentGuideFineTuning/Qwen3-Embedding-8B"

tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, local_files_only=True)
tokenizer.padding_side = "right"
tokenizer.pad_token = tokenizer.eos_token  # Qwen æœ‰äº›æ¨¡å‹æ²¡ pad_token

model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    use_cache=False,
    local_files_only=True
)
model.gradient_checkpointing_enable()
model.config.use_cache = False
model.config.pad_token_id = tokenizer.pad_token_id

# ğŸ” å¯é€‰ï¼šæŸ¥çœ‹æ¨¡å—åï¼ˆè°ƒè¯• LoRAï¼‰
# for name, module in model.named_modules():
#     print(name)

# ========== 3. é…ç½® LoRA ==========
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "up_proj", "down_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    modules_to_save=["lm_head"]
)
model = get_peft_model(model, lora_config)

# ========== 4. åŠ è½½ POI æ•°æ® ==========
with open("finetune_data_poi.jsonl", "r") as f:
    raw_data = [json.loads(line) for line in f]

examples = [{
    "text": ex["intent"].replace("**Intent Summary:**", "").strip() +
            "\n" + ex["prompt"].strip() +
            "\n" + ex["completion"].strip()
} for ex in raw_data]

dataset = Dataset.from_list(examples)

# ========== 5. åˆ†è¯ ==========
def tokenize(example):
    return tokenizer(example["text"], truncation=True, padding="max_length", max_length=256)

tokenized_dataset = dataset.map(tokenize, batched=False, remove_columns=["text"])

# ========== 6. æ„å»º Dataloader ==========
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
train_loader = DataLoader(tokenized_dataset, batch_size=1, shuffle=True, collate_fn=data_collator)

# ========== 7. ä¼˜åŒ–å™¨ ==========
optimizer = AdamW(
    filter(lambda p: p.requires_grad, model.parameters()),
    lr=5e-5
)
# ========== 8. åŠ é€Ÿå™¨å‡†å¤‡ ==========
model, optimizer, train_loader = accelerator.prepare(model, optimizer, train_loader)

# ========== 9. å¼€å§‹è®­ç»ƒ ==========
log_path = "logs/loss_log_qwen3.csv"
os.makedirs("logs", exist_ok=True)

if accelerator.is_main_process:
    with open(log_path, "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["epoch", "step", "loss"])

model.train()
num_epochs = 3
for epoch in range(num_epochs):
    for step, batch in enumerate(train_loader):
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        if (step + 1) % accelerator.gradient_accumulation_steps == 0:
            optimizer.step()
            optimizer.zero_grad()

        if step % 50 == 0:
            accelerator.print(f"Epoch {epoch}, Step {step}, Loss = {loss.item():.4f}")
            if accelerator.is_main_process:
                with open(log_path, "a", newline="") as f:
                    writer = csv.writer(f)
                    writer.writerow([epoch, step, round(loss.item(), 6)])

# ========== 10. ä¿å­˜æƒé‡ ==========
accelerator.wait_for_everyone()
if accelerator.is_main_process:
    model_dir = "./finetuned_qwen3_embedding_8b"
    unwrapped_model = accelerator.unwrap_model(model)
    state_dict = accelerator.get_state_dict(unwrapped_model)
    unwrapped_model.save_pretrained(model_dir, state_dict=state_dict, safe_serialization=True)
    tokenizer.save_pretrained(model_dir)
    accelerator.print(f"âœ… Model saved to {model_dir}")

# ========== 11. ç®€å•æµ‹è¯• ==========
inputs = tokenizer("Hello world!", return_tensors="pt").to(model.device)
outputs = model(**inputs)
print(outputs.logits.shape)
